name: Bootstrap Windows CI

on:
  push:
    paths:
      - 'compiler/ocaml/tests/**'
      - 'tooling/ci/**'
      - 'reports/**'
      - '.github/workflows/bootstrap-windows.yml'
  pull_request:
    paths:
      - 'compiler/ocaml/tests/**'
      - 'tooling/ci/**'
      - 'reports/**'
      - '.github/workflows/bootstrap-windows.yml'
  workflow_dispatch:

jobs:
  diagnostic-json:
    name: Diagnostic JSON Validation (Windows)
    runs-on: windows-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install client compatibility dependencies
        shell: bash
        run: npm ci --prefix tooling/lsp/tests/client_compat

      - name: Run client compatibility tests
        shell: bash
        run: npm run ci --prefix tooling/lsp/tests/client_compat

      - name: Validate diagnostic JSON (Windows)
        shell: bash
        run: bash scripts/validate-diagnostic-json.sh

  audit:
    name: Windows Audit Metrics
    runs-on: windows-latest
    needs:
      - diagnostic-json

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Prepare directories
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p build tooling/ci reports

      - name: Collect iterator/FFI audit metrics (Windows)
        shell: bash
        run: |
          set -euo pipefail
          python3 tooling/ci/collect-iterator-audit-metrics.py \
            --source compiler/ocaml/tests/golden/typeclass_iterator_stage_mismatch.json.golden \
            --source compiler/ocaml/tests/golden/diagnostics/ffi/unsupported-abi.json.golden \
            --audit-source compiler/ocaml/tests/golden/audit/cli-ffi-bridge-windows.jsonl.golden \
            --output tooling/ci/iterator-audit-metrics.json

      - name: Prepare verify log placeholder
        shell: bash
        run: |
          set -euo pipefail
          printf 'Verification succeeded (placeholder for Windows audit validation)\n' > build/verify.log

      - name: Run Windows audit summary
        shell: bash
        run: |
          set -euo pipefail
          bash tooling/ci/sync-iterator-audit.sh \
            --metrics tooling/ci/iterator-audit-metrics.json \
            --verify-log build/verify.log \
            --audit compiler/ocaml/tests/golden/audit/effects-stage.json.golden \
            --output reports/iterator-stage-summary-windows.md

      - name: Upload Windows audit summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: windows-iterator-audit-summary
          path: reports/iterator-stage-summary-windows.md
          retention-days: 14

      - name: Upload Windows audit metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: windows-iterator-audit-metrics
          path: tooling/ci/iterator-audit-metrics.json
          retention-days: 14
